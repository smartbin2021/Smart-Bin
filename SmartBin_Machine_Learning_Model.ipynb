{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Senior_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smartbin2021/Smart-Bin/blob/main/Senior_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elCt-JLMycft"
      },
      "source": [
        "!pip install keras-ocr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qZ3Ex_iqNc5"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv0dgrquyx_C"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import keras_ocr\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        " \n",
        "# show an image\n",
        "import PIL\n",
        "from PIL import ImageDraw\n",
        "from PIL import Image\n",
        "from numpy import asarray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNrle0SAzGVC"
      },
      "source": [
        "remove_dict = {ord('['): None,\n",
        "               ord(']'): None,\n",
        "               ord('/'): None,\n",
        "               ord('\\\\'):None,\n",
        "               ord('*'): None,\n",
        "               ord('$'): None,\n",
        "               ord('&'): None,\n",
        "               ord('!'): None,\n",
        "               ord('@'): None,\n",
        "               ord('#'): None,\n",
        "               ord('%'): None,\n",
        "               ord('^'): None,\n",
        "               ord('('): None,\n",
        "               ord(')'): None,\n",
        "               ord('.'): None,\n",
        "               ord(','): None,\n",
        "               ord(';'): None,\n",
        "               ord(':'): None,\n",
        "               ord('<'): None,\n",
        "               ord('>'): None,\n",
        "               ord('?'): None,\n",
        "               ord('~'): None}\n",
        "\n",
        "word_dict = {'brand_rec':['pepsi', 'lumiere','gardenia','almaza','nutella','chtoura','puidor','rim','aquafina','johnnie','laziza',\n",
        "                          'beirut','dollys','sunbay','pril','dimex','poppins','lipton','perrier','walker','aquafina'],\n",
        "             \n",
        "             'brand_non_rec':['master','cheetos','ringo','lays','pringles','dolsi','snickers', 'mars', 'kitkat','bounty','flake',\n",
        "                              'halabi','twix','bubbly','flake','oreo'],\n",
        "\n",
        "             'brand_limit':['nestle','milka','maccaw','plein','soleil','siblou','americana','alwadi','alakhdar','maxims',\n",
        "                            'dairy','yamama','aruba'],\n",
        "\n",
        "             'word_limit':['coffee','cafe','vinegar','kinder','coco','lait','hot','chili','paste'],\n",
        "\n",
        "             'word_rec_match':['white','red','pops','mate','tomato','life','fish','fillets'],\n",
        "\n",
        "             'word_nonrec_match':['bueno','orange','melk','frozen','milk','chunky','leo','shrimps','rings'],\n",
        "\n",
        "             'word_rec':['beer', 'water','sardines','tuna','mayonnaise', 'ketchup','mustard', 'sanitizer','disinfectant','dip','aluminum',\n",
        "                         'jam','whisky','wine','vodka','flakes','mushrooms','peas','hommus','sauce','scotch','cereal', 'beverage', 'halawa','tea'],\n",
        "\n",
        "             'word_non_rec':['bag', 'chips','potato','wipes','soup','leo','soup','bar']} \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50k9VTl4zLr0"
      },
      "source": [
        "path = \"/content/gdrive/MyDrive/test_images\"\n",
        "#dir_list = [path+'/'+img for img in os.listdir(path) if img.split(\".\")[-1]=='jpg']\n",
        "dir_list = [path+'/'+img for img in os.listdir(path)]\n",
        "print(len(dir_list))\n",
        "dir_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeFatj0q5xGb"
      },
      "source": [
        "pipeline = keras_ocr.pipeline.Pipeline()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBUeCjtvFTmK"
      },
      "source": [
        "def process_image_ocr(image_path):\n",
        "  \"\"\"\n",
        "  Takes an image file path and returns a list of the image in 4 different rotations as numpy array.\n",
        "  \"\"\"\n",
        "  images_list = []\n",
        "  angle = 0\n",
        "  for rotations in range(4):\n",
        "    # Read in an image file\n",
        "    image = Image.open(image_path)\n",
        "    #Rotate image\n",
        "    image = image.rotate(angle)\n",
        "    # Turn the jpeg image into numerical numpy array\n",
        "    image = asarray(image)\n",
        "    #Update angle\n",
        "    angle+=90\n",
        "    #Append image to images list\n",
        "    images_list.append(image)\n",
        "  #Read images using the keras-ocr tool\n",
        "  #images = [keras_ocr.tools.read(img) for img in images_list]\n",
        "  return images_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si1zL2fkUQmt"
      },
      "source": [
        "# Create a function to load a trained model\n",
        "def load_model(model_path):\n",
        "  \"\"\"\n",
        "  Loads a saved model from a specified path.\n",
        "  \"\"\"\n",
        "  print(f\"Loading saved model from: {model_path}\")\n",
        "  model = tf.keras.models.load_model(model_path, custom_objects={\"KerasLayer\":hub.KerasLayer})\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yanRlWaqU8GP"
      },
      "source": [
        "classif_model = load_model('/content/gdrive/MyDrive/20210521-sp2-Cnn_Model__transfer_535 (1).h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0Gg_h39VR-k"
      },
      "source": [
        "# Define image size\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Create a function for preprocessing images\n",
        "def process_image(image_path, img_size=IMG_SIZE):\n",
        "  \"\"\"\n",
        "  Takes an image file path and turns the image into a Tensor.\n",
        "  \"\"\"\n",
        "  # Read in an image file\n",
        "  image = tf.io.read_file(image_path)\n",
        "  # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  # Convert the colour channel values from 0-255 to 0-1 values\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  # Resize the image to our desired value \"IMG_SIZE\"\n",
        "  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\n",
        "  #Expand dimensions\n",
        "  image = np.expand_dims(image,axis=0)\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3yfrN7JnYww"
      },
      "source": [
        "def get_predictions(img_path):\n",
        "  en_bounds = []\n",
        "  ar_bounds = []\n",
        "  en_b = []\n",
        "  ar_b = []\n",
        "  #Extracting images from file paths for Keras-OCR and EasyOCR\n",
        "  images = process_image_ocr(img_path)\n",
        "  #Extracting english text using keras-ocr\n",
        "  keras_images = [keras_ocr.tools.read(img) for img in images]\n",
        "  keras_preds = pipeline.recognize(keras_images)\n",
        "    \n",
        "  #Defining empty list which contains the extracted words\n",
        "  preds = []\n",
        "\n",
        "  #Transforming keras-ocr bounds nested lists into one list which is preds\n",
        "  for pred in keras_preds:\n",
        "    for word in pred:\n",
        "      preds.append(word[0].lower())\n",
        "  \n",
        "\n",
        "  #Remove duplicates from prediction list   \n",
        "  preds = list(set(preds))\n",
        "  return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXrDoWXNDc4m"
      },
      "source": [
        "preds = get_predictions(dir_list[0])\n",
        "preds "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTrovt7p5zGe"
      },
      "source": [
        "def predict(img_path):\n",
        "  #Get predictions from models \n",
        "  preds = get_predictions(img_path)\n",
        "\n",
        "  #See if the words in pred match any of our pre-defined dictionary words\n",
        "  result = None\n",
        "  limit_word = None\n",
        "  for word in preds: \n",
        "    if (word in word_dict[\"brand_rec\"]) or (word in word_dict[\"word_rec\"]):\n",
        "      result = True\n",
        "      return result, word\n",
        "    elif (word in word_dict[\"brand_non_rec\"]) or (word in word_dict[\"word_non_rec\"]):\n",
        "      result = False\n",
        "      return result, word\n",
        "  for word in preds:\n",
        "    if (word in word_dict[\"brand_limit\"]) or (word in word_dict[\"word_limit\"]):\n",
        "      limit_word = word\n",
        "      for match in preds:\n",
        "        if match in word_dict[\"word_rec_match\"]:\n",
        "          result = True\n",
        "          word = word + ', ' +match\n",
        "          return result, word\n",
        "        elif match in word_dict[\"word_nonrec_match\"]:\n",
        "          word = word + ', ' +match\n",
        "          result = False\n",
        "          return result, word\n",
        "  classif_pred = classif_model.predict(process_image(img_path, img_size=IMG_SIZE))\n",
        "  if classif_pred<=0.5:\n",
        "    result = True\n",
        "  else: \n",
        "    result = False\n",
        "  limit_word = \"Classifier\"\n",
        "\n",
        "  return result, limit_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s7U8VxO54bP"
      },
      "source": [
        "def plot_prediction(img_path):\n",
        "  new_path = img_path.split(\"/\")[-1]\n",
        "  new_path = new_path.split(\".\")[0]\n",
        "  result, word = predict(img_path)\n",
        "  txt = \"\"\n",
        "  # 0 is for recyclable, 1 is for non_recyclable, 2 is for unknown, 3 is for limited\n",
        "  product_type= None\n",
        "  if result==True:\n",
        "    txt = \"Recyclable: \" + word\n",
        "    new_path = \"/content/gdrive/MyDrive/Smartbin1/recyclable/\"+ new_path +\"_result.png\"\n",
        "    product_type=0\n",
        "  elif result==False:\n",
        "    txt = \"Non-recyclable: \" + word\n",
        "    new_path = \"/content/gdrive/MyDrive/Smartbin1/non_recyclable/\"+ new_path +\"_result.png\"\n",
        "    product_type=1\n",
        "  elif result==None and word!=None:\n",
        "    txt = \"Unknown: \"+ word\n",
        "    new_path = \"/content/gdrive/MyDrive/Smartbin1/other/\"+ new_path +\"_result.png\"\n",
        "    product_type=2\n",
        "  else:\n",
        "    txt = \"Unknown: No word match\"\n",
        "    new_path = \"/content/gdrive/MyDrive/Smartbin1/other/\"+ new_path +\"_result.png\"\n",
        "    product_type=3\n",
        "  img = mpimg.imread(img_path)\n",
        "  imgplot = plt.imshow(img)\n",
        "  plt.axis('off')\n",
        "  plt.savefig(new_path, bbox_inches='tight')\n",
        "  plt.text(0, -20, txt)\n",
        "  plt.show()\n",
        "  return product_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duyuY0O06BeI"
      },
      "source": [
        "rec_count = 0\n",
        "non_rec_count = 0\n",
        "unknown_count = 0\n",
        "limited_count = 0\n",
        "for img_path in dir_list:\n",
        "  product_type=plot_prediction(img_path)\n",
        "  if product_type==0:\n",
        "    rec_count +=1\n",
        "  elif product_type==1:\n",
        "    non_rec_count +=1\n",
        "  elif product_type==2:\n",
        "    unknown_count +=1\n",
        "  else:\n",
        "    limited_count +=1\n",
        "\n",
        "print(\"Recyclable: \", rec_count)\n",
        "print(\"Non-Recyclable: \", non_rec_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa-df3do-Qph"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
